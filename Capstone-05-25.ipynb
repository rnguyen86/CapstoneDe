{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e46a5a",
   "metadata": {},
   "source": [
    "# Snowboard_haus\n",
    "\n",
    "Snowboard Haus is a premier online retailer that's been curated with top of the line equipments and apparels.  We are committed to providing quality goods and top notch customer service at the lowest prices anywhere.  Our goal is to become the single-source experts on everything snowboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d64dec",
   "metadata": {},
   "source": [
    "# Business Process Flow\n",
    "\n",
    "Below is the proposed business operational flow\n",
    "\n",
    "![business_process_model](mockbusinessmodel.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be4d9e2",
   "metadata": {},
   "source": [
    "# Entity Relationship Diagram\n",
    "\n",
    "Our database consists on the following consumer, transactional, and employee data\n",
    "\n",
    "![business_process_model](mockERD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7408234e",
   "metadata": {},
   "source": [
    "## Snowboard Haus Database DDL\n",
    "\n",
    "The database DDL was constructed and normalized from an the above ERD. The source database was ingested with a collection of mock data generated by Faker.py, API calls from YELP, and webscraping data from the online retailer website, evo.com\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a66a0d7",
   "metadata": {},
   "source": [
    "CREATE DATABASE snowboard_haus\n",
    "\n",
    "CREATE TABLE `payments` (\n",
    "  `customer_id` varchar(255) NOT NULL,\n",
    "  `payment_id` varchar(255) NOT NULL,\n",
    "  `payment_date` DATE NOT NULL,\n",
    "  `price_paid` FLOAT DEFAULT NULL,\n",
    "PRIMARY KEY(`customer_id` , `payment_id`),\n",
    "CONSTRAINT `payments_fk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`customer_id`)\n",
    ");\n",
    "\n",
    "CREATE TABLE `departments` (\n",
    "  `department_no` varchar(255) NOT NULL,\n",
    "  `department_name` varchar(255) NOT NULL,\n",
    "  `department_manager` varchar(255) NOT NULL,\n",
    "PRIMARY KEY (`department_no`)\n",
    ");\n",
    "\n",
    "CREATE TABLE `employees` (\n",
    "  `employee_id` varchar(255) NOT NULL,\n",
    "  `first_name` varchar(255) NOT NULL,\n",
    "  `last_name`varchar(255) NOT NULL,\n",
    "  `email` varchar(255) NOT NULL,\n",
    "  `job_title` varchar(255) NOT NULL,\n",
    "  `department_no` varchar(255) DEFAULT NULL,\n",
    "PRIMARY KEY (`employee_id`),\n",
    "KEY `department_no` (`department_no`),\n",
    "CONSTRAINT `employees_fk_1` FOREIGN KEY (`department_no`) REFERENCES `departments` (`department_no`)\n",
    ");\n",
    "\n",
    "\n",
    "CREATE TABLE `customers` (\n",
    "  `customer_id`varchar(255) NOT NULL,\n",
    "  `first_name` varchar(255) NOT NULL,\n",
    "  `last_name`varchar(255) NOT NULL,\n",
    "  `email` varchar(255) NOT NULL,\n",
    "  `address` varchar(255) NOT NULL,\n",
    "  `city` varchar(255) NOT NULL,\n",
    "  `state` varchar(255) DEFAULT NULL,\n",
    "  `zip` varchar(255) NOT NULL,\n",
    "  `country` varchar(255) NOT NULL,\n",
    "PRIMARY KEY (`customer_id`)\n",
    ");\n",
    "\n",
    "CREATE TABLE `orders` (\n",
    "  `order_id` varchar(255) NOT NULL,\n",
    "  `order_date` DATE NOT NULL,\n",
    "  `shipped_date` DATE DEFAULT NULL,\n",
    "  `status` varchar(255) NOT NULL,\n",
    "  `comments` text,\n",
    "  `customer_id` varchar(255) NOT NULL,\n",
    "PRIMARY KEY(`order_id`),\n",
    "KEY `customer_id` (`customer_id`),\n",
    "CONSTRAINT `orders_fk_1` FOREIGN KEY (`customer_id`) REFERENCES `customers` (`customer_id`)\n",
    "\n",
    ");\n",
    "\n",
    "\n",
    "CREATE TABLE `products` ( \n",
    "  `product_id` varchar(255) NOT NULL,\n",
    "  `product_type` varchar(255) NOT NULL,\n",
    "  `product_name` varchar(255) NOT NULL,\n",
    "  `product_price`  FLOAT NOT NULL,\n",
    "  `stock_quantity` smallint NOT NULL,\n",
    "PRIMARY KEY (`product_id`)\n",
    ");\n",
    "\n",
    "CREATE TABLE `orders_details` (\n",
    "  `order_id` varchar(255) NOT NULL,\n",
    "  `product_id` varchar(255) NOT NULL,\n",
    "  `quantity_ordered` varchar(255) NOT NULL,\n",
    "  `price_paid` FLOAT NOT NULL,\n",
    "PRIMARY KEY(`order_id` , `product_id`),\n",
    "KEY `product_id` (`product_id`),\n",
    "CONSTRAINT `ordersdetail_fk_1` FOREIGN KEY (`order_id`) REFERENCES `orders` (`order_id`),\n",
    "CONSTRAINT `ordersdetail_fk_2` FOREIGN KEY (`product_id`) REFERENCES `products` (`product_id`)\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b33ae",
   "metadata": {},
   "source": [
    "## Connection to source Database"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74b0f3ab",
   "metadata": {},
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def table_to_upload(file_path):\n",
    "\n",
    "    file_to_upload_df = pd.read_csv(file_path)\n",
    "    return file_to_upload_df\n",
    "\n",
    "\n",
    "connection = create_engine('URI, USERNAME, PASSWORD',\n",
    "                           echo=False)\n",
    "\n",
    "upload_df = table_to_upload('csv_folders/payments.csv')\n",
    "\n",
    "upload_df.to_sql('payments', con=connection, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db362b1",
   "metadata": {},
   "source": [
    "## generate_ids.py\n",
    "\n",
    "Used to generate fake ID numbers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "456e1add",
   "metadata": {},
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "def generate_ids():\n",
    "\n",
    "    uid = ''.join(random.choices(string.ascii_uppercase + string.digits, k=10))\n",
    "    return uid\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e9511a",
   "metadata": {},
   "source": [
    "## web_scrape.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7040a43",
   "metadata": {},
   "source": [
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from capstoneDE.generate_ids import generate_ids\n",
    "\n",
    "\n",
    "def webscrape_product_data(url_list):\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for url in url_list:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        product_items = soup.find_all('div', class_='product-thumb-details', limit=20)\n",
    "        product_type = soup.find ('h1', class_='js-fixed-header').text\n",
    "\n",
    "\n",
    "        for item in product_items:\n",
    "            product_id = generate_ids()\n",
    "            product_type = product_type\n",
    "            product_name = item.find ('span', class_='product-thumb-title').text.replace ('\\n', '')\n",
    "            product_price = item.find ('span', class_='product-thumb-price').text.replace ('\\n', '')\n",
    "            stock_quantity = random.randint(0, 100)\n",
    "            data.append([product_id, product_type, product_name, product_price, stock_quantity])\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3075729b",
   "metadata": {},
   "source": [
    "## yelp_api_key.py\n",
    "Yelp's API Key"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f829e0fc",
   "metadata": {},
   "source": [
    "YELP_KEY = 'APIkey'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82c222f",
   "metadata": {},
   "source": [
    "## yelp_api_location.py\n",
    "\n",
    "making API calls to collect business ID numbers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d9616ee9",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "# Define a business ID\n",
    "# Define API Key, Search Type, and header\n",
    "MY_API_KEY = 'WspvpJMsPlCo2m_bYmh2q3ODy3DwvNK7J41c8r32J4Jx7YEDbwMNlY9dHqiHuF6qIsgOoRXR3DlULSrTBnrgmaMYvH8IUnNNrO98IVa058IrWE4ODCusUx-CF0ZyY3Yx'\n",
    "BUSINESS_PATH = f'https://api.yelp.com/v3/businesses/search'\n",
    "HEADERS = {'Authorization': 'bearer %s' % MY_API_KEY}\n",
    "\n",
    "# Define the Parameters of the search\n",
    "PARAMETERS = {'locale':'en_US',\n",
    "              'term': 'snowboard shop',\n",
    "              'limit': 50,\n",
    "              'radius': 40000,\n",
    "              'location': 'colorado'\n",
    "              }\n",
    "\n",
    "response = requests.get(url=BUSINESS_PATH,\n",
    "                        params=PARAMETERS,\n",
    "                        headers=HEADERS)\n",
    "\n",
    "# Convert response to a JSON String\n",
    "\n",
    "business_data = response.json()\n",
    "\n",
    "loc_busi = business_data['businesses'][:]\n",
    "\n",
    "#saving each business ID to a list\n",
    "loc_ids = []\n",
    "for links in loc_busi:\n",
    "    loc_ids.append(links['id'])\n",
    "\n",
    "print(loc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748f0c50",
   "metadata": {},
   "source": [
    "## yelp_api_call.py\n",
    "\n",
    "Makes a yelp API call to collect reviews from different retail shops around Denver,CO"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e959ca7a",
   "metadata": {},
   "source": [
    "import requests\n",
    "import json\n",
    "from yelp_api_key import YELP_KEY\n",
    "from yelp_api_location import loc_ids\n",
    "\n",
    "MY_API_KEY = YELP_KEY\n",
    "HEADERS = {'Authorization': 'bearer %s' % MY_API_KEY}\n",
    "PARAMETERS = {'locale': 'en_US'\n",
    "              }\n",
    "#calling each business IDs and extracting 3 reviews\n",
    "comments = []\n",
    "for id in loc_ids:\n",
    "    response = requests.get(url='https://api.yelp.com/v3/businesses/'+ id +'/reviews',\n",
    "                             params=PARAMETERS,\n",
    "                             headers=HEADERS)\n",
    "    business_data = response.json()\n",
    "    data = business_data['reviews']\n",
    "    for x in data:\n",
    "        quotes = (x['text'])\n",
    "        comments.append(quotes)\n",
    "\n",
    "\n",
    "#saving the reviews to a json file\n",
    "f = open ('yelp_quotes.json', 'w')\n",
    "f.write (json.dumps (reviews, indent=3))\n",
    "f.close ()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d9ed6b",
   "metadata": {},
   "source": [
    "## customers.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "617ce8a7",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "from capstoneDE.generate_ids import generate_ids\n",
    "\n",
    "# columns I need:\n",
    "# “firstname”, “lastname”, “street address”, “city”, “state”, “zip”, “phone”, and “email_address”\n",
    "\n",
    "\n",
    "def fake_data_generation(records):\n",
    "    fake = Faker('en_US')\n",
    "\n",
    "    customers = []\n",
    "\n",
    "    for i in range(records):\n",
    "        first_name = fake.first_name()\n",
    "        last_name = fake.last_name()\n",
    "\n",
    "        customers.append({\n",
    "            \"customer_id\": generate_ids(),\n",
    "            \"first_name\": first_name,\n",
    "            \"last_name\": last_name,\n",
    "            \"phone\": fake.phone_number(),\n",
    "            \"email\": str.lower(f\"{first_name}.{last_name}@fake_domain.com\"),\n",
    "            \"address\": fake.street_address(),\n",
    "            \"city\": fake.city(),\n",
    "            \"state\": fake.state_abbr(),\n",
    "            \"zip\": fake.postcode(),\n",
    "            \"country\": fake.country_code()\n",
    "        })\n",
    "\n",
    "    return customers\n",
    "\n",
    "\n",
    "customer_df = pd.DataFrame(fake_data_generation(500))\n",
    "customer_table = customer_df.to_csv('../csv_folders/customer.csv',index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ea5010",
   "metadata": {},
   "source": [
    "## departments.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "115a670f",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "department_data = [{'department_no': '1001',\n",
    "                    'department_name': 'sales',\n",
    "                    'department_manager': 'Rich Win'},\n",
    "                   {'department_no': '1002',\n",
    "                    'department_name': 'marketing',\n",
    "                    'department_manager': 'Joe Doe'},\n",
    "                   {'department_no': '1003',\n",
    "                    'department_name': 'IT',\n",
    "                    'department_manager': 'Mike Jordan'},\n",
    "                   {'department_no': '1004',\n",
    "                    'department_name': 'business development',\n",
    "                    'department_manager': 'Nyjah Huston'},\n",
    "                   {'department_no': '1005',\n",
    "                    'department_name': 'analytics',\n",
    "                    'department_manager': 'James Beard'}\n",
    "                   ]\n",
    "\n",
    "department_df = pd.DataFrame(department_data)\n",
    "department_table = department_df.to_csv('../csv_folders/departments.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f827a618",
   "metadata": {},
   "source": [
    "## employees.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "79cda68a",
   "metadata": {},
   "source": [
    "import random\n",
    "from capstoneDE.generate_ids import generate_ids\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "\n",
    "\n",
    "sales_jobs = ['account representative', 'account executive', 'sales consultant', 'salesperson']\n",
    "marketing_jobs = ['marketing assistant', 'brand ambassador', 'content specialist', 'SEO specialist', 'social media manager']\n",
    "it_jobs = ['solutions architect', 'web developer', 'data engineer', 'network engineer', 'service desk engineer']\n",
    "analyst_jobs = ['data analyst', 'business intelligence analyst', 'R&D', 'consultant']\n",
    "bd_jobs = ['accountant', 'project manager', 'human resources', 'business analyst']\n",
    "\n",
    "def fake_employee_generation(records):\n",
    "    fake = Faker('en_US')\n",
    "\n",
    "    employees = []\n",
    "\n",
    "    for i in range(records):\n",
    "        first_name = fake.first_name()\n",
    "        last_name = fake.last_name()\n",
    "\n",
    "        employees.append({\n",
    "            \"employee_id\": generate_ids(),\n",
    "            \"first_name\": first_name,\n",
    "            \"last_name\": last_name,\n",
    "            \"email\": str.lower(f\"{first_name}.{last_name}@fake_domain.com\"),\n",
    "            \"department_no\": random.randint(1001, 1005)\n",
    "        })\n",
    "\n",
    "    return employees\n",
    "\n",
    "\n",
    "def decide_job(row):\n",
    "\n",
    "    if row['department_no'] == 1001:\n",
    "        return random.choice(sales_jobs)\n",
    "    elif row['department_no'] == 1002:\n",
    "        return random.choice(marketing_jobs)\n",
    "    elif row['department_no'] == 1003:\n",
    "        return random.choice(it_jobs)\n",
    "    elif row['department_no'] == 1004:\n",
    "        return random.choice(bd_jobs)\n",
    "    elif row['department_no'] == 1005:\n",
    "        return random.choice(analyst_jobs)\n",
    "\n",
    "\n",
    "employees_df = pd.DataFrame(fake_employee_generation(200))\n",
    "employees_df['job_title'] = employees_df.apply(lambda row: decide_job(row), axis=1)\n",
    "\n",
    "employees_csv = employees_df.to_csv('../csv_folders/employees.csv', index=False)\n",
    "\n",
    "print(employees_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d68657f",
   "metadata": {},
   "source": [
    "## order_details.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0c4366b",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "orders_df = pd.read_csv('../csv_folders/orders.csv')\n",
    "products_df = pd.read_csv('../csv_folders/product_table.csv')\n",
    "\n",
    "# order_id, product_id, quantity_ordered, price_paid\n",
    "\n",
    "order_details_df = pd.DataFrame(columns=['order_id', 'product_id', 'quantity_ordered', 'price_paid'])\n",
    "\n",
    "order_details_df['order_id'] = orders_df['order_id']\n",
    "\n",
    "order_details_df['product_id'] = products_df['product_id'].sample(n=750, replace=True, ignore_index=True)\n",
    "\n",
    "order_details_df['quantity_ordered'] = np.random.randint(1, 10, size=len(order_details_df))\n",
    "\n",
    "order_details_table = order_details_df.to_csv('../csv_folders/order_details.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ff4efa",
   "metadata": {},
   "source": [
    "## orders.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fef3e0c1",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from capstoneDE.generate_ids import generate_ids\n",
    "from faker import Faker\n",
    "from datetime import datetime\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "\n",
    "f = open('../yelp_quotes.json')\n",
    "data = json.load(f)\n",
    "customer_df = pd.read_csv('../csv_folders/customer.csv')\n",
    "\n",
    "\n",
    "\n",
    "def fake_order_generation(records):\n",
    "    fake = Faker('en_US')\n",
    "    orders = []\n",
    "\n",
    "    for i in range(records):\n",
    "\n",
    "        orders.append({\n",
    "            \"order_id\": generate_ids(),\n",
    "            \"order_date\": fake.date_between_dates(date_start=datetime(2022,10,1), date_end=datetime(2022,11,1)),\n",
    "            \"shipped_date\": fake.date_between_dates(date_start=datetime(2022,10,2), date_end=datetime(2022,11,30)),\n",
    "            \"status\": 'shipped',\n",
    "            \"comments\": random.choice(data),\n",
    "        })\n",
    "\n",
    "    return orders\n",
    "\n",
    "f.close()\n",
    "\n",
    "order_df = pd.DataFrame(fake_order_generation(500))\n",
    "order_df['comments'] = order_df['comments'].sample(frac=.8).astype(str)\n",
    "order_df['customer_id'] = customer_df['customer_id'].sample(n=750, replace=True, ignore_index=True)\n",
    "\n",
    "order_df[['order_date', 'shipped_date']] = order_df[['order_date', 'shipped_date']].astype(str)\n",
    "\n",
    "orders_table = order_df.to_csv('../csv_folders/orders.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79513a7a",
   "metadata": {},
   "source": [
    "## payments.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1013e503",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from capstoneDE.generate_ids import generate_ids\n",
    "\n",
    "payment_id_list = []\n",
    "orders_df = pd.read_csv('../csv_folders/orders.csv')\n",
    "\n",
    "\n",
    "for i in range(750):\n",
    "    payment_id = generate_ids()\n",
    "    payment_id_list.append(payment_id)\n",
    "\n",
    "#paymentDate, price_paid will come from SQL\n",
    "payments_df = pd.DataFrame(data=payment_id_list, columns=['payment_id'])\n",
    "payments_df['customer_id'] = orders_df['customer_id']\n",
    "\n",
    "\n",
    "payments_table = payments_df.to_csv('../csv_folders/payments.csv', index=False)\n",
    "print(payments_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0d0d27",
   "metadata": {},
   "source": [
    "## products.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "405ce86a",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from capstoneDE.web_scrape import webscrape_product_data\n",
    "\n",
    "\n",
    "product_url_list = ['https://www.evo.com/shop/snowboard/bags/new',\n",
    "                    'https://www.evo.com/shop/snowboard/bindings/new',\n",
    "                    'https://www.evo.com/shop/snowboard/tuning-tools',\n",
    "                    'https://www.evo.com/shop/snowboard/boots/new',\n",
    "                    'https://www.evo.com/shop/snowboard/facemasks/new',\n",
    "                    'https://www.evo.com/shop/snowboard/goggles/new',\n",
    "                    'https://www.evo.com/shop/snowboard/helmets/new',\n",
    "                    'https://www.evo.com/shop/snowboard/jackets/new',\n",
    "                    'https://www.evo.com/shop/snowboard/pants/mens/womens/new',\n",
    "                    'https://www.evo.com/shop/snowboard/snowboards/boys',\n",
    "                    'https://www.evo.com/shop/snowboard/snowboards/girls',\n",
    "                    'https://www.evo.com/shop/snowboard/snowboards/kids',\n",
    "                    'https://www.evo.com/shop/snowboard/snowboards/womens/new']\n",
    "\n",
    "\n",
    "def create_product_df(data_list):\n",
    "\n",
    "    df = pd.DataFrame(data=data_list,\n",
    "                      columns=['product_id','product_type','product_name','product_price','stock_quantity'])\n",
    "    df['product_price'] = df['product_price'].str.replace('$', '')\n",
    "    df[['product_type', 'product_name', 'product_price']] = df[['product_type', 'product_name', 'product_price']]. \\\n",
    "        apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "product_df = create_product_df(webscrape_product_data(product_url_list))\n",
    "\n",
    "product_table = product_df.to_csv('../csv_folders/product_table.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d53f21",
   "metadata": {},
   "source": [
    "Data Lake created using AWS Lake Formation and ETL Glue crawlers. All necessary prerequisites and security protocols configured.\n",
    "\n",
    "Data catalog is hosted on administrator account. All raw data was verified and transformed with the following Glue jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08486d57",
   "metadata": {},
   "source": [
    "## Transform and verify data\n",
    "\n",
    "Used  Glue ETL job to correct mapping errors and converted CSV to Parquet files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad7864a",
   "metadata": {},
   "source": [
    "## Orders verified"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da0af914",
   "metadata": {},
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "args = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args[\"JOB_NAME\"], args)\n",
    "\n",
    "# Script generated for node S3 bucket\n",
    "S3bucket_node1 = glueContext.create_dynamic_frame.from_options(\n",
    "    format_options={\n",
    "        \"quoteChar\": '\"',\n",
    "        \"withHeader\": True,\n",
    "        \"separator\": \",\",\n",
    "        \"optimizePerformance\": False,\n",
    "    },\n",
    "    connection_type=\"s3\",\n",
    "    format=\"csv\",\n",
    "    connection_options={\n",
    "        \"paths\": [\"s3://capstone-05-25-rtn/raw_data/orders/\"],\n",
    "        \"recurse\": True,\n",
    "    },\n",
    "    transformation_ctx=\"S3bucket_node1\",\n",
    ")\n",
    "\n",
    "# Script generated for node ApplyMapping\n",
    "ApplyMapping_node2 = ApplyMapping.apply(\n",
    "    frame=S3bucket_node1,\n",
    "    mappings=[\n",
    "        (\"col0\", \"string\", \"order_date\", \"date\"),\n",
    "        (\"col1\", \"string\", \"comments\", \"string\"),\n",
    "        (\"col2\", \"string\", \"shipped_date\", \"date\"),\n",
    "        (\"col3\", \"string\", \"customer_id\", \"string\"),\n",
    "        (\"col4\", \"string\", \"order_id\", \"string\"),\n",
    "        (\"col5\", \"string\", \"status\", \"string\"),\n",
    "    ],\n",
    "    transformation_ctx=\"ApplyMapping_node2\",\n",
    ")\n",
    "\n",
    "# Script generated for node S3 bucket\n",
    "S3bucket_node3 = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=ApplyMapping_node2,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"csv\",\n",
    "    connection_options={\n",
    "        \"path\": \"s3://capstone-05-25-rtn/verified_data/orders/\",\n",
    "        \"partitionKeys\": [],\n",
    "    },\n",
    "    transformation_ctx=\"S3bucket_node3\",\n",
    ")\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c36e68a",
   "metadata": {},
   "source": [
    "## Payments verified"
   ]
  },
  {
   "cell_type": "raw",
   "id": "899b9c24",
   "metadata": {},
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "args = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args[\"JOB_NAME\"], args)\n",
    "\n",
    "# Script generated for node S3 bucket\n",
    "S3bucket_node1 = glueContext.create_dynamic_frame.from_options(\n",
    "    format_options={\n",
    "        \"quoteChar\": '\"',\n",
    "        \"withHeader\": True,\n",
    "        \"separator\": \",\",\n",
    "        \"optimizePerformance\": False,\n",
    "    },\n",
    "    connection_type=\"s3\",\n",
    "    format=\"csv\",\n",
    "    connection_options={\n",
    "        \"paths\": [\"s3://capstone-05-25-rtn/raw_data/payments/\"],\n",
    "        \"recurse\": True,\n",
    "    },\n",
    "    transformation_ctx=\"S3bucket_node1\",\n",
    ")\n",
    "\n",
    "# Script generated for node ApplyMapping\n",
    "ApplyMapping_node2 = ApplyMapping.apply(\n",
    "    frame=S3bucket_node1,\n",
    "    mappings=[\n",
    "        (\"price_paid\", \"string\", \"price_paid\", \"string\"),\n",
    "        (\"payment_id\", \"string\", \"payment_id\", \"string\"),\n",
    "        (\"customer_id\", \"string\", \"customer_id\", \"string\"),\n",
    "        (\"payment_date\", \"string\", \"payment_date\", \"string\"),\n",
    "    ],\n",
    "    transformation_ctx=\"ApplyMapping_node2\",\n",
    ")\n",
    "\n",
    "# Script generated for node S3 bucket\n",
    "S3bucket_node3 = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=ApplyMapping_node2,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"glueparquet\",\n",
    "    connection_options={\n",
    "        \"path\": \"s3://capstone-05-25-rtn/verified_data/payments/\",\n",
    "        \"partitionKeys\": [],\n",
    "    },\n",
    "    transformation_ctx=\"S3bucket_node3\",\n",
    ")\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfac8305",
   "metadata": {},
   "source": [
    "## Order details verified"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65e249e5",
   "metadata": {},
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "args = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args[\"JOB_NAME\"], args)\n",
    "\n",
    "# Script generated for node S3 bucket\n",
    "S3bucket_node1 = glueContext.create_dynamic_frame.from_options(\n",
    "    format_options={\n",
    "        \"quoteChar\": '\"',\n",
    "        \"withHeader\": True,\n",
    "        \"separator\": \",\",\n",
    "        \"optimizePerformance\": False,\n",
    "    },\n",
    "    connection_type=\"s3\",\n",
    "    format=\"csv\",\n",
    "    connection_options={\n",
    "        \"paths\": [\"s3://capstone-05-25-rtn/raw_data/order_details/\"],\n",
    "        \"recurse\": True,\n",
    "    },\n",
    "    transformation_ctx=\"S3bucket_node1\",\n",
    ")\n",
    "\n",
    "# Script generated for node ApplyMapping\n",
    "ApplyMapping_node2 = ApplyMapping.apply(\n",
    "    frame=S3bucket_node1,\n",
    "    mappings=[\n",
    "        (\"quantity_ordered\", \"string\", \"quantity_ordered\", \"string\"),\n",
    "        (\"price_paid\", \"string\", \"price_paid\", \"string\"),\n",
    "        (\"product_id\", \"string\", \"product_id\", \"string\"),\n",
    "        (\"order_id\", \"string\", \"order_id\", \"string\"),\n",
    "    ],\n",
    "    transformation_ctx=\"ApplyMapping_node2\",\n",
    ")\n",
    "\n",
    "# Script generated for node S3 bucket\n",
    "S3bucket_node3 = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=ApplyMapping_node2,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"glueparquet\",\n",
    "    connection_options={\n",
    "        \"path\": \"s3://capstone-05-25-rtn/verified_data/order_details/\",\n",
    "        \"partitionKeys\": [],\n",
    "    },\n",
    "    transformation_ctx=\"S3bucket_node3\",\n",
    ")\n",
    "\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6164b35a",
   "metadata": {},
   "source": [
    "## Employees verified"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15996260",
   "metadata": {},
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "args = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args[\"JOB_NAME\"], args)\n",
    "\n",
    "# Script generated for node S3 bucket\n",
    "S3bucket_node1 = glueContext.create_dynamic_frame.from_options(\n",
    "    format_options={\n",
    "        \"quoteChar\": '\"',\n",
    "        \"withHeader\": True,\n",
    "        \"separator\": \",\",\n",
    "        \"optimizePerformance\": False,\n",
    "    },\n",
    "    connection_type=\"s3\",\n",
    "    format=\"csv\",\n",
    "    connection_options={\n",
    "        \"paths\": [\"s3://capstone-05-25-rtn/raw_data/employees/\"],\n",
    "        \"recurse\": True,\n",
    "    },\n",
    "    transformation_ctx=\"S3bucket_node1\",\n",
    ")\n",
    "\n",
    "# Script generated for node ApplyMapping\n",
    "ApplyMapping_node2 = ApplyMapping.apply(\n",
    "    frame=S3bucket_node1,\n",
    "    mappings=[\n",
    "        (\"department_no\", \"string\", \"department_no\", \"string\"),\n",
    "        (\"employee_id\", \"string\", \"employee_id\", \"string\"),\n",
    "        (\"last_name\", \"string\", \"last_name\", \"string\"),\n",
    "        (\"first_name\", \"string\", \"first_name\", \"string\"),\n",
    "        (\"job_title\", \"string\", \"job_title\", \"string\"),\n",
    "        (\"email\", \"string\", \"email\", \"string\"),\n",
    "    ],\n",
    "    transformation_ctx=\"ApplyMapping_node2\",\n",
    ")\n",
    "\n",
    "# Script generated for node S3 bucket\n",
    "S3bucket_node3 = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=ApplyMapping_node2,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"glueparquet\",\n",
    "    connection_options={\n",
    "        \"path\": \"s3://capstone-05-25-rtn/verified_data/employees/\",\n",
    "        \"partitionKeys\": [],\n",
    "    },\n",
    "    transformation_ctx=\"S3bucket_node3\",\n",
    ")\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf06933",
   "metadata": {},
   "source": [
    "## Departments verified"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52740906",
   "metadata": {},
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "args = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args[\"JOB_NAME\"], args)\n",
    "\n",
    "# Script generated for node S3 bucket\n",
    "S3bucket_node1 = glueContext.create_dynamic_frame.from_options(\n",
    "    format_options={\n",
    "        \"quoteChar\": '\"',\n",
    "        \"withHeader\": True,\n",
    "        \"separator\": \",\",\n",
    "        \"optimizePerformance\": False,\n",
    "    },\n",
    "    connection_type=\"s3\",\n",
    "    format=\"csv\",\n",
    "    connection_options={\n",
    "        \"paths\": [\"s3://capstone-05-25-rtn/raw_data/departments/\"],\n",
    "        \"recurse\": True,\n",
    "    },\n",
    "    transformation_ctx=\"S3bucket_node1\",\n",
    ")\n",
    "\n",
    "# Script generated for node ApplyMapping\n",
    "ApplyMapping_node2 = ApplyMapping.apply(\n",
    "    frame=S3bucket_node1,\n",
    "    mappings=[\n",
    "        (\"department_no\", \"string\", \"department_no\", \"string\"),\n",
    "        (\"department_name\", \"string\", \"department_name\", \"string\"),\n",
    "        (\"department_manager\", \"string\", \"department_manager\", \"string\"),\n",
    "    ],\n",
    "    transformation_ctx=\"ApplyMapping_node2\",\n",
    ")\n",
    "\n",
    "# Script generated for node S3 bucket\n",
    "S3bucket_node3 = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=ApplyMapping_node2,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"glueparquet\",\n",
    "    connection_options={\n",
    "        \"path\": \"s3://capstone-05-25-rtn/verified_data/departments/\",\n",
    "        \"partitionKeys\": [],\n",
    "    },\n",
    "    transformation_ctx=\"S3bucket_node3\",\n",
    ")\n",
    "\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330fe854",
   "metadata": {},
   "source": [
    "## Customers verified"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8af39b96",
   "metadata": {},
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "args = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args[\"JOB_NAME\"], args)\n",
    "\n",
    "# Script generated for node S3 bucket\n",
    "S3bucket_node1 = glueContext.create_dynamic_frame.from_options(\n",
    "    format_options={\n",
    "        \"quoteChar\": '\"',\n",
    "        \"withHeader\": True,\n",
    "        \"separator\": \",\",\n",
    "        \"optimizePerformance\": False,\n",
    "    },\n",
    "    connection_type=\"s3\",\n",
    "    format=\"csv\",\n",
    "    connection_options={\n",
    "        \"paths\": [\"s3://capstone-05-25-rtn/raw_data/customers/\"],\n",
    "        \"recurse\": True,\n",
    "    },\n",
    "    transformation_ctx=\"S3bucket_node1\",\n",
    ")\n",
    "\n",
    "# Script generated for node ApplyMapping\n",
    "ApplyMapping_node2 = ApplyMapping.apply(\n",
    "    frame=S3bucket_node1,\n",
    "    mappings=[\n",
    "        (\"zip\", \"string\", \"zip\", \"string\"),\n",
    "        (\"country\", \"string\", \"country\", \"string\"),\n",
    "        (\"address\", \"string\", \"address\", \"string\"),\n",
    "        (\"city\", \"string\", \"city\", \"string\"),\n",
    "        (\"phone\", \"string\", \"phone\", \"string\"),\n",
    "        (\"last_name\", \"string\", \"last_name\", \"string\"),\n",
    "        (\"state\", \"string\", \"state\", \"string\"),\n",
    "        (\"customer_id\", \"string\", \"customer_id\", \"string\"),\n",
    "        (\"first_name\", \"string\", \"first_name\", \"string\"),\n",
    "        (\"email\", \"string\", \"email\", \"string\"),\n",
    "    ],\n",
    "    transformation_ctx=\"ApplyMapping_node2\",\n",
    ")\n",
    "\n",
    "# Script generated for node S3 bucket\n",
    "S3bucket_node3 = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=ApplyMapping_node2,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"glueparquet\",\n",
    "    connection_options={\n",
    "        \"path\": \"s3://capstone-05-25-rtn/verified_data/customers/\",\n",
    "        \"partitionKeys\": [],\n",
    "    },\n",
    "    transformation_ctx=\"S3bucket_node3\",\n",
    ")\n",
    "\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec02248",
   "metadata": {},
   "source": [
    "## Products verified"
   ]
  },
  {
   "cell_type": "raw",
   "id": "afa15325",
   "metadata": {},
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "args = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args[\"JOB_NAME\"], args)\n",
    "\n",
    "# Script generated for node S3 bucket\n",
    "S3bucket_node1 = glueContext.create_dynamic_frame.from_options(\n",
    "    format_options={\n",
    "        \"quoteChar\": '\"',\n",
    "        \"withHeader\": True,\n",
    "        \"separator\": \",\",\n",
    "        \"optimizePerformance\": False,\n",
    "    },\n",
    "    connection_type=\"s3\",\n",
    "    format=\"csv\",\n",
    "    connection_options={\n",
    "        \"paths\": [\"s3://capstone-05-25-rtn/raw_data/products/\"],\n",
    "        \"recurse\": True,\n",
    "    },\n",
    "    transformation_ctx=\"S3bucket_node1\",\n",
    ")\n",
    "\n",
    "# Script generated for node ApplyMapping\n",
    "ApplyMapping_node2 = ApplyMapping.apply(\n",
    "    frame=S3bucket_node1,\n",
    "    mappings=[\n",
    "        (\"product_type\", \"string\", \"product_type\", \"string\"),\n",
    "        (\"product_id\", \"string\", \"product_id\", \"string\"),\n",
    "        (\"product_price\", \"string\", \"product_price\", \"string\"),\n",
    "        (\"stock_quantity\", \"string\", \"stock_quantity\", \"string\"),\n",
    "        (\"product_name\", \"string\", \"product_name\", \"string\"),\n",
    "    ],\n",
    "    transformation_ctx=\"ApplyMapping_node2\",\n",
    ")\n",
    "\n",
    "# Script generated for node S3 bucket\n",
    "S3bucket_node3 = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=ApplyMapping_node2,\n",
    "    connection_type=\"s3\",\n",
    "    format=\"glueparquet\",\n",
    "    connection_options={\n",
    "        \"path\": \"s3://capstone-05-25-rtn/verified_data/products/\",\n",
    "        \"partitionKeys\": [],\n",
    "    },\n",
    "    format_options={\"compression\": \"uncompressed\"},\n",
    "    transformation_ctx=\"S3bucket_node3\",\n",
    ")\n",
    "\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe098288",
   "metadata": {},
   "source": [
    "## Redshift Warehouse DDL"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b67c525",
   "metadata": {},
   "source": [
    "create external schema capstone0525\n",
    "from data catalog\n",
    "database 'dev'\n",
    "iam_role 'arn:aws:iam::820723595533:role/capstone-redshift-role'\n",
    "create external database if not exists;\n",
    "\n",
    "\n",
    "\n",
    "create external table capstone0525.departments(\n",
    "  department_no varchar(255),\n",
    "  department_name varchar(255),\n",
    "  department_manager varchar(255)\n",
    ")\n",
    "stored as parquet\n",
    "location 's3://capstone-05-25-rtn/verified_data/departments/';\n",
    "\n",
    "\n",
    "CREATE external TABLE capstone0525.payments (\n",
    "  customer_id varchar(255),\n",
    "  payment_id varchar(255),\n",
    "  payment_date varchar(255),\n",
    "  price_paid varchar(255)\n",
    ")\n",
    "stored as parquet\n",
    "location 's3://capstone-05-25-rtn/verified_data/payments/';\n",
    "\n",
    "\n",
    "CREATE external TABLE capstone0525.employees(\n",
    "  employee_id varchar(255),\n",
    "  first_name varchar(255),\n",
    "  last_name varchar(255),\n",
    "  email varchar(255),\n",
    "  job_title varchar(255),\n",
    "  department_no varchar(255)\n",
    ")\n",
    "stored as parquet\n",
    "location 's3://capstone-05-25-rtn/verified_data/employees/';\n",
    "\n",
    "\n",
    "CREATE external TABLE capstone0525.customers (\n",
    "  customer_id varchar(255),\n",
    "  first_name varchar(255),\n",
    "  last_name varchar(255),\n",
    "  email varchar(255),\n",
    "  address varchar(255),\n",
    "  city varchar(255),\n",
    "  state varchar(255),\n",
    "  zip varchar(255),\n",
    "  country varchar(255)\n",
    ")\n",
    "stored as parquet\n",
    "location 's3://capstone-05-25-rtn/verified_data/customers/';\n",
    "\n",
    "CREATE external TABLE capstone0525.products ( \n",
    "  product_id varchar(255),\n",
    "  product_type varchar(255),\n",
    "  product_name varchar(255),\n",
    "  product_price varchar(255),\n",
    "  stock_quantity varchar(255) \n",
    ")\n",
    "stored as parquet\n",
    "location 's3://capstone-05-25-rtn/verified_data/products/';\n",
    "\n",
    "\n",
    "CREATE external TABLE capstone0525.order_details (\n",
    "  order_id varchar(255),\n",
    " product_id varchar(255),\n",
    "  quantity_ordered varchar(255),\n",
    " price_paid varchar(255)\n",
    "  )\n",
    "stored as parquet\n",
    "location 's3://capstone-05-25-rtn/verified_data/order_details/';\n",
    "  \n",
    "\n",
    "create external table capstone0525.orders(\n",
    "  order_date DATE,\n",
    "  comments varchar(255),\n",
    "  shipped_date DATE,\n",
    "  customer_id varchar(255),\n",
    "  order_id varchar(255),\n",
    "  status varchar(255)\n",
    ")\n",
    "stored as csv\n",
    "location 's3://capstone-05-25-rtn/verified_data/orders/';\n",
    "\n",
    "\n",
    "select * from capstone0525.departments;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a594fdcc",
   "metadata": {},
   "source": [
    "## Screenshots of verified data on Redshift\n",
    "## Departments\n",
    "![departments_verified](departments-verified.png)\n",
    "## Payments\n",
    "![departments_verified](payments-verified.png)\n",
    "## Employees\n",
    "![departments_verified](employees-verified.png)\n",
    "## Customers\n",
    "![departments_verified](customers-verified.png)\n",
    "## Products\n",
    "![departments_verified](products-verified.png)\n",
    "## Order Details\n",
    "![departments_verified](order-details-verified.png)\n",
    "## Orders\n",
    "![departments_verified](orders-verified.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bd8e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab7a706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
